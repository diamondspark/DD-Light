global_params:
  expt_name: '8FinalDock' #'It will create wandb experiment with this name
  code_path: '/groups/cherkasvgrp/tsajed/ddlight' # path to the ddlight folder, need to change
  project_path: '/groups/cherkasvgrp/tsajed/ddlight' # All output files and results
  project_name: 'cache-7-pgk2-4-iter-10M-lib-vina' # Multiple ddlight experiments Name of the protein. All run will be saved under there
  dock_pgm: 'vina' #'glide' or 'vina'
  topK : 2000 # At the end of ddrun number of best molecules
  grid_file: '/groups/cherkasvgrp/DeepDocking_Projects/BLAR_2M/glide-grid_blar/glide-grid_blar.zip' # Customize where the grid file.
  glide_input_template: '/groups/cherkasvgrp/share/progressive_docking/DD_dev_slurm/glide_input_ex.in' # Customize where the glide input template; not changed across projects. fixed for lab
  dataset_path: 'dataset/Enamine10M.pkl' # Path to the dataset of SMILES
  # db_path: '/groups/cherkasvgrp/Student_backup/mkpandey/My_Projects/DDSgroups/Enamine6_75_sparse.db'
  model_architecture: 'advanced_molformer' #'mlpTx' #'molformer' # keep this fixed for now
  target: 'pgk2' #'mpro' #'mt1r' #'mpro' #'2bm2' # key to docking grid, needs to be pdb id
  # descriptors: 'ECFP' no need to use this now
  # descriptor_dir: '/groups/cherkasvgrp/Student_backup/mkpandey/My_Projects/DDSgroups/data/preprocessed/descriptors'
  schrodinger_path: '/groups/cherkasvgrp/schrodinger' # Path to the schrodinger installation; fixed in our lab
  # consolidated_fp_path: '/groups/cherkasvgrp/Student_backup/mkpandey/My_Projects/DDSgroups/data/preprocessed/descriptors/consolidated_ECFP6_75M.pkl'
  env_name: 'dds'
  loss: 'crossentropy' # focal loss
  dock_logs_path: '/groups/cherkasvgrp/tsajed/ddlight/dock_logs'
al_params:
  acquisition_function: 'bald' #"unc" # #least_confidence #'random' # "entropy", "least_confidence", "margin", "greedy", "ucb", .
  initial_budget: 5000 #00 #20000 #10000 #1000000 # First round of iterations for train
  initial_val_budget: 5000 # Total validation budget
  initial_test_budget: 5000 # Total test budget
  al_budget: 1000 #00 #10000 # 1000 # Number of molecules to acquire at each iteration
  n_iterations: 4 # Number of active learning iterations
  model_retrain: False #True # whether to transfer learn or retrain from scratch; comment out in prod
  test_rand_samples: False # At every iteration, do you want to randomly choose molecules to test docking score
  drop_db : False # Iteratively making the dataset more manageable. Drop most informative inactives
  drop_db_recall: True # If drop_db = True, if recall goes up, then only drop informative inactives
  subset_inference: True # If True, subset the inference to 10M molecules for each iteration
model_hps:
  num_gpus: 200 # Parallel training on num gpus
  num_layers: 6 # Number of layers in the model
  n_epochs: 40 # Number of epochs to train the model
  lr:  0.0001 #0.000001 # 0.0001
  batch_size: 2048 # Batch size for training
  optimizer: "adam" # Optimizer
  random_seed: 42 # Random seed for reproducibility
  patience: 3 # Patience for early stopping
threshold_params:
  fixed_cutoff: True #-9.6 #-8.58 #-10.0 # Whether to use a fixed cutoff for the docking score, The percentage does not change. If not true, dynamic percentage.
  given_cutoff: null #-10 IF true, docking score exact cutoff
